{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmation GPU - Projet #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, uint32\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel CUDA pour FNV-1a ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def fnv1a_kernel(messages, lengths, output):\n",
    "    FNV_OFFSET_BASIS = uint32(0x811c9dc5)\n",
    "    FNV_PRIME = uint32(0x01000193)\n",
    "\n",
    "    idx = cuda.grid(1)\n",
    "    if idx >= messages.shape[0]:\n",
    "        return\n",
    "\n",
    "    h = FNV_OFFSET_BASIS\n",
    "    for i in range(lengths[idx]):\n",
    "        h ^= messages[idx, i]\n",
    "        h = uint32(h * FNV_PRIME)  # ✅ FIX ICI\n",
    "\n",
    "    output[idx] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d’appel CPU → GPU ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_all_gpu(input_list):\n",
    "    \"\"\"\n",
    "    Hash une liste de chaînes avec FNV-1a sur GPU (Numba CUDA)\n",
    "    \"\"\"\n",
    "    print(f\"Hash de {len(input_list):,} chaînes en GPU (FNV-1a)...\")\n",
    "\n",
    "    n = len(input_list)\n",
    "    max_len = 32  # on suppose que les chaînes font au max 32 caractères\n",
    "\n",
    "    # Préparation des données : tableau 2D de uint8\n",
    "    messages_np = np.zeros((n, max_len), dtype=np.uint8)\n",
    "    lengths_np = np.zeros(n, dtype=np.uint8)\n",
    "\n",
    "    for i, s in enumerate(input_list):\n",
    "        encoded = s.encode('utf-8')[:max_len]\n",
    "        messages_np[i, :len(encoded)] = list(encoded)\n",
    "        lengths_np[i] = len(encoded)\n",
    "\n",
    "    output_np = np.zeros(n, dtype=np.uint32)\n",
    "\n",
    "    # Allocation et copie sur GPU\n",
    "    d_messages = cuda.to_device(messages_np)\n",
    "    d_lengths = cuda.to_device(lengths_np)\n",
    "    d_output = cuda.device_array(n, dtype=np.uint32)\n",
    "\n",
    "    threads_per_block = 128\n",
    "    blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "    # Lancement du kernel\n",
    "    start = time.time()\n",
    "    fnv1a_kernel[blocks_per_grid, threads_per_block](d_messages, d_lengths, d_output)\n",
    "    cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    # Récupération des résultats\n",
    "    d_output.copy_to_host(ary=output_np)\n",
    "\n",
    "    print(f\"Temps total : {end - start:.4f} secondes\")\n",
    "    print(f\"Exemple : {input_list[0]} -> {output_np[0]:08x}\")\n",
    "    return output_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test principal ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_data = [f\"data_{i}\" for i in range(1_000_000)]  # 1 million\n",
    "    hashes_gpu = hash_all_gpu(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
